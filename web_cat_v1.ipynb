{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "app = Flask(__name__)\n",
    "df=pickle.load(open('catalog.pkl','rb'))\n",
    "\n",
    "@app.route('/', methods=['POST','GET'])    \n",
    "def my_form_post():\n",
    "    \n",
    "    a='' \n",
    "    \n",
    "    if request.method==\"POST\":\n",
    "        a=request.form.get('name2')\n",
    "        \n",
    "    qs = a.lower().strip().split()\n",
    "\n",
    "    match = lambda s: sum(min(3, s.lower().count(qp)) for qp in qs)\n",
    "    pairs = []\n",
    "    for pid, p in df['NAME'].items():\n",
    "        score = 0.0\n",
    "        score += 1.0 * match(p)\n",
    "        if score > 0:\n",
    "            pairs.append((score, pid))\n",
    "\n",
    "    pairs.sort(reverse=True)\n",
    "    pids = [p[1] for p in pairs]\n",
    "    scores = [p[0] for p in pairs]\n",
    "\n",
    "    d1=df.iloc[pids][['CODE','NAME']].reset_index()\n",
    "\n",
    "    code=d1['CODE']\n",
    "    name=d1['NAME']\n",
    "    score=scores\n",
    "\n",
    "    keys=['name','code','score']\n",
    "    values=[name,code,score]\n",
    "\n",
    "    newList = []\n",
    "    for i in range(len(values[0])):\n",
    "\n",
    "        tmp = {}\n",
    "        for j, key in enumerate(keys):\n",
    "            tmp[key] = values[j][i]\n",
    "\n",
    "        newList.append(tmp)\n",
    "\n",
    "    return render_template('aa2.html',data=newList)        \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run(host='0.0.0.0', port=82, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "#from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "patterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "#stopwords_ru = stopwords.words(\"russian\")\n",
    "stopwords_ru=['другои', 'еи', 'какои', 'мои', 'неи', 'сеичас', 'такои', 'этои','и', 'в', 'во', 'не', 'что', 'он','на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне','было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "\n",
    "\n",
    "def lemmatize(doc):\n",
    "    doc = re.sub(patterns, ' ', doc)\n",
    "    tokens = []\n",
    "    for token in doc.split():\n",
    "        if token and token not in stopwords_ru:\n",
    "            token = token.strip()\n",
    "            token = morph.normal_forms(token)[0]\n",
    "            \n",
    "            tokens.append(token.lower())\n",
    "    if len(tokens) > 0:\n",
    "        return tokens\n",
    "    return None\n",
    "\n",
    "df=pd.read_excel('catalog_v1.xlsx',sheet_name='1')\n",
    "\n",
    "data=df['NAME'].apply(lemmatize).str.join(' ').tolist()\n",
    "\n",
    "\n",
    "v = TfidfVectorizer(input='content',\n",
    "                    encoding='utf-8', decode_error='replace', strip_accents='unicode',\n",
    "                    lowercase=True, analyzer='word', stop_words=stopwords_ru,\n",
    "                    #token_pattern=r'(?u)\\b[а-яА-Я_][а-яА-Я0-9_]+\\b',\n",
    "                    #ngram_range=(1, 2),\n",
    "                    ngram_range=(1, 1),\n",
    "                    max_features=20000,\n",
    "                    norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=True,\n",
    "                    max_df=0.1, min_df=5\n",
    "                    #max_df=1, min_df=1\n",
    "                   )\n",
    "\n",
    "tfidf_vectorizer_vectors=v.fit_transform(data)\n",
    "\n",
    "data_temp=pd.DataFrame(tfidf_vectorizer_vectors.T.todense(),index=v.get_feature_names_out())#[1].sort_values(ascending=False).head(30)\n",
    "\n",
    "import pickle\n",
    "pickle.dump(data_temp,open('catalog_vec_tfidf.pkl','wb'))\n",
    "pickle.dump(v,open('model_tfidf.pkl','wb'))\n",
    "pickle.dump(df,open('catalog.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "import pickle\n",
    "app = Flask(__name__)\n",
    "\n",
    "data_temp=pickle.load(open('catalog_vec_tfidf.pkl','rb'))\n",
    "v=pickle.load(open('model_tfidf.pkl','rb'))\n",
    "df=pickle.load(open('catalog.pkl','rb'))\n",
    "    \n",
    "def result_v1(search_text,data_temp=None,v=None,df=None):\n",
    "\n",
    "    from pymorphy2 import MorphAnalyzer\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import re\n",
    "    \n",
    "    patterns = \"[A-Za-z0-9!#$%&'()*+,./:;<=>?@[\\]^_`{|}~—\\\"\\-]+\"\n",
    "    #stopwords_ru = stopwords.words(\"russian\")\n",
    "    stopwords_ru=['другои', 'еи', 'какои', 'мои', 'неи', 'сеичас', 'такои', 'этои','и', 'в', 'во', 'не', 'что', 'он','на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне','было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n",
    "    morph = MorphAnalyzer()\n",
    "\n",
    "    def lemmatize(doc):\n",
    "        doc = re.sub(patterns, ' ', doc)\n",
    "        tokens = []\n",
    "        for token in doc.split():\n",
    "            if token and token not in stopwords_ru:\n",
    "                token = token.strip()\n",
    "                token = morph.normal_forms(token)[0]\n",
    "\n",
    "                tokens.append(token.lower())\n",
    "        if len(tokens) > 0:\n",
    "            return tokens\n",
    "        return None\n",
    "\n",
    "\n",
    "    a=[search_text]\n",
    "    a=[' '.join(pd.Series(a).apply(lemmatize)[0])]\n",
    "    #print(a)\n",
    "    t=[cosine_similarity(np.array(v.transform(a).T.todense()).reshape(1, -1), np.array((data_temp[i])).reshape(1, -1))[0][0] for i in range(data_temp.shape[1])]\n",
    "    t2=pd.DataFrame([t,df['CODE'],df['NAME']]).T.rename(columns={0:'SCORE',1:'CODE',2:'NAME'}).sort_values('SCORE',ascending=False).head(10)[['CODE','NAME','SCORE']]\n",
    "    return t2['CODE'].tolist(),t2['NAME'].tolist(),t2['SCORE'].tolist()\n",
    "    \n",
    "@app.route('/', methods=['POST','GET'])    \n",
    "def my_form_post():\n",
    "\n",
    "    \n",
    "    a='пример' \n",
    "\n",
    "    #newList=[{'name': 'red', 'code': 'a1', 'score': 1}, {'name': 'green', 'code': 'a5', 'score': 2}, {'name': 'blue', 'code': 'a11', 'score': 3}, {'name': 'blue2', 'code': 'a2', 'score': 5}]\n",
    "    \n",
    "    if request.method==\"POST\":\n",
    "        a=request.form.get('name2')\n",
    "        \n",
    "    #a='молоко мячи всякое разно мебели'\n",
    "    code,name,score=result_v1(a,data_temp,v,df)\n",
    "\n",
    "    keys=['name','code','score']\n",
    "    values=[name,code,score]\n",
    "\n",
    "    newList = []\n",
    "    for i in range(len(values[0])):\n",
    "\n",
    "        tmp = {}\n",
    "        for j, key in enumerate(keys):\n",
    "            tmp[key] = values[j][i]\n",
    "\n",
    "        newList.append(tmp)\n",
    "    \n",
    "\n",
    "    return render_template('aa2.html',data=newList)        \n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    app.run(host='0.0.0.0', port=81, debug=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
